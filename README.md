# SPED7500-project
This project tests the viability of conceptual features as a route to actionable learning analytics. Following work on explainable modeling, we probe the feasibility of using human-in-the-loop linguistic modeling to construct conceptual features that can aid in instructors' interpretation of predictive models and provide useful pointers as to the kind of support students need.

To assess the viability of this approach, we present a proof-of-concept study that developed predictive analytics for students' failure attribution, which aims to inform instructors in providing targeted support for students' failure recovery. The work was conducted in the context of dental students’ competency exam reflections where students failing an exam were asked to complete written reflections on the causes of failure and how they planned to address them. In this context, students’ causal attributions can be linguistically modeled based on the words they used to describe their perceived reasons for failure. Explainable AI methods provide further opportunities to build transparent models that share the linguistic features contributing to the final prediction. However, sharing low-level linguistic features (e.g., n-grams) can be difficult for instructors to interpret and plan for targeted support. This study thus examined whether conceptual features describing students’ perceived causes of failure can be constructed via [human-in-the-loop linguistic modeling](https://github.com/Fanjie-Li/SPED7500-project/blob/main/1-Feature_construction.ipynb). 

For validation of the constructed features, we trained and evaluated a [random forest classifier using the proposed conceptual features](https://github.com/Fanjie-Li/SPED7500-project/blob/main/2-Classification-conceptual.ipynb) and compared its performance with the [classifier built on the baseline n-grams features](https://github.com/Fanjie-Li/SPED7500-project/blob/main/3-Classification-ngrams.ipynb). The links above can be used as quick access to Jupyter Notebooks for feature construction and classification experiments.

## Data
Although we cannot share the reflection corpus due to data sharing restrictions, this GitHub repository provides access to the conceptual features and n-grams extracted from our corpus. For context, the reflection statements used in this study are students’ reflections on failed clinical and non-clinical competency exams. When students fail one of these exams, they are asked to respond to two reflection prompts, i.e., (1) "What did I do incorrectly that caused me to fail this exam?" and (2) "What can I do to pass the exam next time?".

## Jupyter Notebooks
Steps for constructing conceptual features are described in the [1-Feature_construction](https://github.com/Fanjie-Li/SPED7500-project/blob/main/1-Feature_construction.ipynb) Jupyter Notebook, which outlines how we extracted phrases from the reflection corpus for identifying linguistic patterns associated with different perceived reasons for failure and the resulting patterns for extracting conceptual features.

The [2-Classification-conceptual](https://github.com/Fanjie-Li/SPED7500-project/blob/main/2-Classification-conceptual.ipynb) and [3-Classification-ngrams](https://github.com/Fanjie-Li/SPED7500-project/blob/main/3-Classification-ngrams.ipynb) documented the procedures and configurations of our classification experiments, which includes data preparation, model training and evaluation, and model interpretation using the local interpretable model-agnostic explanations (LIME) method.
